\chapter{Context}

\section{Concepte din algebra liniară}

\begin{defn}
  Un \textbf{monoid} $G = (A, \circ)$ este o structură algebrică formată dintr-o
  mulțime $A$ și o lege de compoziție asociativă și cu element neutru.
\end{defn}

\begin{defn}
  Un \textbf{grup} $G = (A, \circ)$ este o structură algebrică formată dintr-o
  mulțime $A$ și o lege de compoziție care respectă următoarele proprietăți:
  \begin{itemize}
      \item{$\forall\ x_{1}, x_{2} \in G, x_{1} \circ x_{2} \in G$.}
      \item{$\forall\ x_{1}, x_{2}, x_{3} \in G, (x_{1} \circ x_{2}) \circ x_{3} = x_{1} \circ (x_{2} \circ x_{3})$.}
      \item{$\exists\ e \in G \text{ astfel
      încât } \forall\ x \in G\ e \circ x = x \circ e = x$.}
      \item{$\forall\ x_{1} \in G\ \exists\ x_{2} \in G \ x_{1} \circ x_{2} = x_{2} \circ x_{1} = e$.}
  \end{itemize}
  În plus, acesta este abelian dacă
  $\forall\ x_{1}, x_{2} \in G\ x_{1} \circ x_{2} = x_{2} \circ x_{1}$.
\end{defn}

\begin{defn}
  Un \textbf{inel} $I = (A, +, *)$ este o structura algebrică formată dintr-o
  mulțime $A$ și două operații binare definite pe $A \times A$ cu valori tot în
  $A$, care respectă următoarele proprietăți:
  \begin{itemize}
      \item{$(A, +)$ este un grup abelian.}
      \item{$(A, *)$ este un monoid.}
      \item{Distributivitatea lui $*$ față de $+$.}
  \end{itemize}
\end{defn}

\begin{defn}
  Un \textbf{polinom} este o expresie construită dintr-una sau mai multe
  variabile și constante, folosind doar operații de adunare, scădere, înmulțire
  și ridicare la putere constanta pozitivă întreagă.
\end{defn}

\begin{defn}
  O \textbf{matrice} este un tabel dreptunghiular de elemente ale unui inel.
\end{defn}

\subsection{Interpolare Lagrange}

\textbf{Polinomul de interpolare Lagrange} este un polinom $P$ construit în așa
fel încât având $k + 1$ perechi
$(x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{k+1}, y_{k+1})$ avem că
$P(x_{i}) = y_{i}\ \forall\ 1 \leq i \leq K + 1$. \par
Construcția lui $P$ a fost oferită de Joseph Louis Lagrange în $1795$:
\begin{equation}
  P(x) := \displaystyle\sum\limits_{i_{1}=0}^{k} (y_{i_{1}} * \prod_{1 \leq i_{2} \leq k + 1 \ \land \ i_{1} \neq i_{2}} \frac{x - x_{i_{2}}}{x_{i_{1}} - x_{i_{2}}})
\end{equation}

\section{Eliminare Gaussiană}
\label{gauss}

O metodă de a rezolva sisteme de ecuații liniare este \textbf{Eliminarea Gaussiană}.
Aceasta se mai numește și metoda \textbf{Gauss-Jordan} pentru că este o variație a
metodei lui Gauss, descrisă de către Jordan în anul 1887.

Algoritmul face eliminări secvențiale a tuturor variabilelor până când fiecare ecuație
rămâne cu o singură variabilă. Dacă o privim ca o ecuație $Ax = b$, atunci se poate
vedea că, în cazul în care $A$ este o matrice pătratică, atunci vrem să o transformăm
în matricea identitate și să rezolvăm sistemul pe acest caz trivial.

La primul pas, algoritmul împarte prima linie cu $a_{1, 1}$ și apoi adaugă o combinație
liniară a primei ecuații tuturor celorlalte astfel încât toți ceilalți coeficienți de pe
prima coloană să fie $0$; mai exact pentru rândul $a_{i}, i \geq 2$, vom adaugă primul rând
înmulțit cu $-a_{i}$. Toate aceste operații se execută și pe vectorul coloană $b$. Se
continuă aceleași operații pentru restul liniilor.

Este posibil ca acele valori să fie $0$. De aceea există mai multe euristici, în special gândite
pentru a stabiliza operațiile pe numere reale în virgulă flotantă. Euristica cea mai des folosită
în practică este să se caute valoarea cea mai mare absolută de pe acea coloană și să se interschimbe
cele două rânduri.

Complexitatea algoritmului este de $O(N^{2}M)$ dacă $A$ este de mărime $N \times M$, cu $N \leq M$.
În practică acest algoritm se poate aplica și atunci când se lucrează într-un corp finit.
Un caz des întâlnit este în $\mathbb{F}_{2}$. Pe modelul RAM acest caz se poate implementa chiar în
complexitate $O(\frac{N^{2}M}{w})$, unde $w$ este mărimea unui cuvânt, adică $64$ pentru procesoarele
moderne.

Metoda poate fi folosită și pentru a calcula inversa unei matrice. Tot ce este de făcut, pentru o matrice
$A$ de mărime $N \times N$ este să se execute algoritmul pe matricea $[A\ |\ I_{N}]$ de mărime $N \times 2N$,
iar matricea de pe blocul din dreapta (coloanele  cu indicemai mare ca $N$) va fi chiar $A^{-1}$.

Pentru calculul determinantului, putem executa algoritmul de înainte cu mențiunea că nu vom mai împărți fiecare
linie, dar vom înmulți acele valori de pivot și de fiecare dată când schimbăm două linii, schimbăm semnul
rezultatului. Dacă la un moment dat nu se găsește un pivot, atunci rezultatul este chiar $0$. Dacă se
dorește calculul rangului, atunci aceste este chiar numărul pivoților diferiți de $0$.

\pagebreak

\section{Teoria grafurilor}

\begin{defn}
  Un \textbf{graf neorientat} este o pereche ordonată $G(V, E)$, unde:
  \begin{itemize}
    \item{$V$ este mulțimea de noduri}.
    \item{$E \subseteq \{(x, y)\ |\ (x, y) \in V^2 \land x \neq y\}$ este
      mulțimea de muchii, care sunt perechi neordonate de noduri}.
  \end{itemize}
\end{defn}

\begin{defn}
  Un \textbf{arbore} este un graf neorientat conex aciclic.
\end{defn}

\begin{defn}
  Un \textbf{graf bipartit} este un graf neorientat $G(V, E)$ ale cărui noduri pot fi
  partiționate în două mulțimi disjuncte $V_{1}$ și $V_{2}$, altfel încât fiecare muchie
  conectează un nod din mulțimea $V_{1}$ cu un nod din mulțimea $V_{2}$. Atunci,
  se va nota cu $G(V_{1}, V_{2}, E)$.
\end{defn}

\begin{defn}
  Un \textbf{cuplaj} într-un graf este o mulțime de muchii fără noduri în comun.
\end{defn}

\begin{defn}
  \textbf{Matricea Edmonds} $A$ a grafului bipartit $G(U, V, E)$, unde
  $U = \{u_{1}, u_{2}, u_{3}, \ldots, u_{n}\}$,
  $V = \{v_{1}, v_{2}, v_{3}, \ldots, v_{n}\}$ și $x_{i,j}$ indeterminate, se definește ca:
   \begin{equation}
    A=
    \begin{cases}
      x_{i,j} & \text{dacă}\ (u_{i}, v_{j}) \in E \\
      0 & \text{altfel}
    \end{cases}
  \end{equation}
\end{defn}

\begin{thm}
  \label{edmonds}
  Un graf bipartit $G(U, V, E)$ admite cuplaj perfect dacă și numai dacă
  polinomul determinantului matricei sale Edmonds nu este polinomul nul.
\end{thm}

\begin{clr}
  Numărul de cuplaje perfecte este egal cu numărul monoamelor în polinomul $det(A_{i,j})$.
\end{clr}

\begin{clr}
  Rangul matricei Edmonds este egal cu mărimea cuplajului maximal.
\end{clr}

\pagebreak

\section{Lema Schwartz–Zippel}
\textbf{Lema Schwartz-Zippel} este o metodă utilă pentru a verifica într-un mod
probabilistic dacă un polinom în mai multe variabile este polinomul nul.

\begin{thm}
  Fie $P \in F \lbrack x_{1}, x_{2}, \ldots, x_{n} \rbrack$ un polinom nenul de grad $d \geq 0$
  peste corpul $F$ și $S$ o submultime finită a lui $F$. Alegem
  $r_{1}, r_{2}, \ldots, r_{n}$ uniform aleator și independent din $S$. Atunci:
  \begin{equation}
    \Pr \lbrack P(r_{1}, r_{2}, \ldots, r_{n}) = 0 \rbrack \leq \frac{d}{|S|}
  \end{equation}
\end{thm}

Acest rezultat are sens odată ce ne gândim pe cazul de baza, când $n=1$ deoarece
un polinom de grad $d$ are cel mult $d$ rădăcini. Întreagă demonstrație se
bazează pe inducție matematică și se poate consulta în \cite{schwartzzippel}.

\begin{thm}
  Lema Schwartz-Zippel se poate folosi în \ref{edmonds} pentru a verifica
  existența unui cuplaj perfect în timp polinomial.
\end{thm}

Fie $A$ \textbf{matricea Edmonds} asociată grafului. $\det(A)$ este un polinom
în $n^{2}$ variabile, de grad $n$. Alegem corpul pe care lucrăm că fiind
$\mathbb{F}_{p}$. Determinantul poate fi calculat sub $\mathbb{F}_{p}$ în timp $O(n^{3})$
folosind Eliminare Gaussiana. Chiar mai mult decât atât, acest calcul
poate fi făcut în timp $O(\log^{2} n)$ dacă avem la dispoziție $O(n^{3.5})$
procesoare \cite{paralleldet}. Probabilitatea de eșec a algoritmului este de $\frac{n}{p}$.

\pagebreak

\section{Lema de izolare}
\textbf{Lema de izolare} este o metodă de a reduce numărul de soluții ale unei
probleme la una singură, dacă aceasta există. Această se obține prin adăugarea
unor constrângeri aleatoare în așa fel încât, cu o probabilitate neglijabilă,
va există o singură soluție care satisface constragerile adiționale.

Lema a fost introdusă de Valiant și Vazirani în lucrarea ``NP is as easy as
detecting unique solutions'', publicată în 1986.

\begin{thm}
  \label{isolation}
  Fie $n$ și $m$ două numere întregi pozitive și $F$ o familie de submultimi a
  lui $\{1, 2, \ldots n\}$. Fie
  $w : \{1, 2, \ldots, n\} \to \{1, 2, \ldots, m\}$ o funcție care asociază
  valori uniform aleatoare și independente din codomeniu. Atunci, cu o
  probabilitate de cel puțin $1 - \frac{n}{m}$, va există o singură mulțime $S$ în
  $F$ pentru care $\displaystyle\sum\limits_{x\in S}{w(x)}$ este minimă.
\end{thm}

\pagebreak

\section{Algoritm paralelizabil pentru cuplaj}
\label{Algoritm paralelizabil pentru cuplaj}

În ``Matching is as easy as matrix inversion'' \cite{matchingezmatrix} se
descrie o metodă pentru a căuta cuplajul perfect folosind acest rezultat.
Considerăm $G(U, V, E)$ un graf bipartit. Asociem fiecărei muchii din $E$ un
cost uniform din mulțimea $\{1, 2, \ldots, 2|E|\}$. Conform \ref{isolation}, cu
probabilitate de cel puțin $\frac{1}{2}$ va exista un cuplaj unic de cost minim.
Considerăm $U = \{u_{1}, u_{2}, u_{3}, \ldots, u_{n}\}$,
$V = \{v_{1}, v_{2}, v_{3}, \ldots, v_{n}\}$ și costul aleator ales mai devreme muchiei
$(u_{i}, v_{j})$ că fiind $w_{i,j}$. Fie $A$ matricea Edmonds a acestui
graf, construită în felul următor:
\begin{equation}
A=
\begin{cases}
  2^{w_{i,j}} & \text{dacă}\ (u_{i}, v_{j}) \in E \\
  0 & \text{altfel}
\end{cases}
\end{equation}

Definim costul unui cuplaj ca fiind suma costurilor muchiilor alese în acea mulțime.
\begin{lem}
  Dacă $p$ este costul cuplajului minim și acesta este și unic, atunci $\det(A) \neq 0$, iar
  $2^{p} \ |\ \det(A)$ și $2^{p+1} \not| \ \det(A)$.
\end{lem}

Fie $\text{sgn} : S_{n} \to \{\pm 1\}$, $\text{sgn}(P)$ este $+1$ dacă $P$
este permutare pară, altfel $-1$. $\text{sgn}(P)$ se numește signatura
permutarii $P$.

\begin{equation}
  \label{isolationdet}
  \det(A) = \displaystyle\sum\limits_{P \in S_{n}} (\text{sgn}(P) * \prod_{1 \leq i \leq N} A_{i, P_{i}})
\end{equation}

Pentru fiecare cuplaj din $G$ putem asocia unic o permutare $P$ din $S_{n}$:
dacă muchia $(u_{i}, v_{j})$ este aleasă în cuplaj, atunci $P_{i} \equiv j$.
Pentru o permutare care nu are asociat un cuplaj perfect, aceasta nu va
contribui deloc la valoarea expresiei \ref{isolationdet}. \par

Dacă $P$ este permutarea asociată cuplajului de cost minim, atunci
$\prod_{1 \leq i \leq N} A_{i, P_{i}} = 2^{p}$. Valoarea celorlalte permutări
este fie $0$, fie o putere de $2$ mai mare decât $2^{p}$ (deoarece $P$ este
unică). Așadar, valoarea determinatului o să fie divizibilă cu $2^{p}$ și
mai mult, această va fi cel mai mare divizor de această formă.

\pagebreak

Pentru a determina mulțimea muchiilor din cuplaj, trebuie să mai facem câteva observații.

\begin{lem}
  Fie $A'$ matricea obținută eliminând linia $i$ și coloana $j$. Fie
  $C \def \det(A') * \frac{2^{w_{i,j}}}$. Muchia
  $(u_{i}, v_{j})$ face parte din cuplaj dacă și numai dacă $2^{p} \ | \ C$,
  $2^{p+1} \not| \ C$ și $C \neq 0$.
\end{lem}

În primul rând, valoarea $\det(A') * 2^{w_{i,j}}$ este chiar valoarea
determinantului calculat după formula de la \ref{isolationdet}, cu precizarea că
se vor considera doar permutările $P$ cu $P_{i} = j$. Acest lucru se datorează
faptului că permutările alese în calculul lui $\det(A')$ vor avea cu un element
mai puțin decât cele din $\det(A)$, iar pentru elementul lipsă înmulțim mereu cu
valorea asociată muchiei $(u_{i}, v_{j})$, adică $2^{w_{i,j}}$.
Aplicând apoi observațiile demonstrației pentru \ref{isolationdet} obținem
rezultatul dorit și în cele din urmă, un algoritm pentru determinarea cuplajului
perfect care nu se bazează pe rețele de flux, ci doar pe obiect de algebră
liniară: \par

\vspace{5 mm}

\begin{algorithm}[H]
 \label{cuplajizolare}
 \KwData{Graf bipartit G(U, V, E) care admite cuplaj perfect}
 \KwResult{Mulțimea muchiilor alese într-un posibil cuplaj perfect}
 \Begin{
  determină w\;
  construiește A\;
  $p \longleftarrow \det(A)$\;
  $C \longleftarrow \{\}$\;
  \For{$(u_{i}, v_{j}) \in E$}{
    determină $A'$\;
    \If{$\frac{\det(A') * 2^{w_{i,j}}}{2^{p}} \mod 2 = 1$}{
      $C \longleftarrow C \cup \{(u_{i}, v_{j})\}$\;
    }
  }
  \Return{$C$}\;
 }
 \caption{Cuplaj perfect lema de izolare}
\end{algorithm}

\pagebreak

În continuare vom presupune că putem efectua operațiile aritmetice de bază în timp $O(1)$.
Atunci algoritmul \ref{cuplajizolare} durează $O(n^{5})$, dacă folosim
algoritmul lui Gauss pentru calcularea determinantului. Putem să nu recalculăm
valoarea determinantului la fiecare iterație folosind:
\begin{equation}
  \det(A + uv^{T}) = (1 + v^{T} A^{-1} u) \det(A)
\end{equation}

Cu această proprietatea a determinantului, cunoscută și ca \textbf{Matrix
  determinant lemma} putem reduce complexitatea la $O(n^{4})$. În plus,
algoritmul se poate paraleliza bine, deoarece determinantul unei matrice
de mărime $n \times n$ se poate calcula în timp $O(\log^{2}n)$ pe $O(n^{3.5})$ procesoare.

Unicitatea soluției, garantata de \textbf{lema de izolare} permite procesoarelor
să execute și pasul următor, în care se testează dacă o muchie face parte din
cuplaj, în paralel, deoarece se îndreaptă către aceeași soluție, cu o
certitudine destul de mare. Așadar și această parte a algoritmului poate fi
calculată în timp polilogaritmic, dacă avem la dispoziție un număr polinomial de
procesoare.

În \cite{optimalmatching} se arată că acest algoritm poate fi rafinat la complexitate
$O(n^{3})$, observandu-se că o muchie $(u_{i}, v_{j})$ face parte din cuplaj dacă și
numai dacă $A^{-1}_{i, j} \neq 0$. Mai mult, folosindu-se de această proprietate și
de o schemă de a reduce eliminarea Gaussiana (\ref{gauss}) la $O(1)$ înmulțiri de matrici,
acest algoritm are complexitate $O(n^{w})$, unde $w < 2.38$, astfel obtinandu-se un
algoritm mai eficient decât cel bazat pe flux maxim, Hopcroft-Karp, care are complexitatea
$O(n^{2.5})$.

\pagebreak

\section{Cuplaj roșu-albastru}
\label{redbluematching}

Să considerăm un graf bipartit $G(U, V, E)$, unde $E_{1}$ este mulțimea
muchiilor roșii, $E_{2}$ este mulțimea muchiilor albastre, iar
$E = E_{1} \cup E_{2}$. Dorim să alfam dacă există un cuplaj perfect
în care se folosesc exact $k$ muchii roșii, știind că dacă acesta există, atunci
este și \textbf{unic}. \par

Aceasta problemă nu se poate rezolva cu algoritmi de flux maxim, spre deosebire
de celelalte probleme de până acum. Totuși, avem șanse să o rezolvam, dacă
procedam prin a modifica \textbf{matricea Edmonds} astfel încât să avem un mod
de a ne da seama de culoarea muchiilor alese în cuplaj:

\begin{equation}
  A=
  \begin{cases}
    y & \text{dacă}\ (u_{i}, v_{j}) \in E_{1} \\
    1 & \text{dacă}\ (u_{i}, v_{j}) \in E_{2} \\
    0 & \text{altfel}
  \end{cases}
\end{equation}

$\det(A)$ va fi un polinom în $y$ de grad $n$. $G$ va avea un cuplaj perfect cu
$k$ muchii roșii dacă coeficientul lui $y^{k}$ va fi nenul. Pentru a determina
acest număr putem să interpolăm polinomul $\det(A)$ folosind interpolarea
Lagrange. Tot ce rămâne de făcut este să evaluăm polinomul în $n+1$ puncte. \par

Pentru cazul general, când cuplajul roșu-albastru nu este neapărat unic, putem
folosi una din cele două leme să reducem problema la una mai simpla.

\subsection{Folosind Lema Schwartz-Zippel}

Pentru fiecare muchie $(u_{i}, v_{j})$ alegem o valoarea aleatoare $x_{i,j}$ dintr-o
mulțimea $S$. Construim matricea astfel:

\begin{equation}
  A=
  \begin{cases}
    y * x_{i,j} & \text{dacă}\ (u_{i}, v_{j}) \in E_{1} \\
    x_{i,j} & \text{dacă}\ (u_{i}, v_{j}) \in E_{2} \\
    0 & \text{altfel}
  \end{cases}
\end{equation}

Ca înainte, vom interpola $\det(A)$ pentru a afla coeficientul lui $y^{k}$.
Acest coeficient trebuie să fie nenul. Iar acum, cu probabilitate mare, acest
lucru este adevărat, pentru că dacă două polinoame se ``anulează'' unul pe
celalalt în urma calculului determinantului, atunci cu probabilitate de cel mult
$\frac{n}{|S|}$ acestea erau diferite.

\subsection{Folosind Lema de izolare}
Asemănător algoritmului prezentat la \ref{Algoritm paralelizabil pentru cuplaj}
și soluției precedente cu Lema Schwartz-Zippel, având valorile $w_{i,j}$ alese
din $\{1, 2, \ldots, 2|E_{1} \cup E_{2}|\}$, vom construi matricea astfel:

\begin{equation}
  A=
  \begin{cases}
    y * 2^{w_{i,j}} & \text{dacă}\ (u_{i}, v_{j}) \in E_{1} \\
    2^{w_{i,j}} & \text{dacă}\ (u_{i}, v_{j}) \in E_{2} \\
    0 & \text{altfel}
  \end{cases}
\end{equation}

În continuare rămâne să ne uităm la coeficientul lui $y^{k}$ în $\det(A)$. Spre
deosebire de lema \textbf{Schwartz-Zippel}, acesta variantă are posibilitatea să fie
paralelizabil, deoarece, cu probabilitate de cel puțin $\frac{1}{2}$, cuplajul
pe care vrem să-l găsim este acum unic.

\pagebreak

\section{Transformări Fourier}

\subsection{Transformarea Fourier Discreta \cite{fft}}
Considerăm $n = 2^{p}$ și următorul polinom de grad $n-1$:
\begin{equation}
  A = a_{0}x^{0} + a_{1}x^{1} + \ldots + a_{n-1}x^{n-1}
\end{equation}

Soluția ecuației $x^{n} = 1$ are $n$ soluții în mulțimea numerelor complexe.
Acestea sunt de forma $w_{p} = e^{\frac{2k\pi i}{n}}$ pentru $p$ un număr întreg
intre $0$ și $n-1$. Aceste rădăcini au multe proprietăți utile, iar cea pe care
urmează să o folosim este că $w_{p} = w_{0}^{p}$. \par
\textbf{Transformarea discretă Fourier} a polinomului $A$ este definită ca fiind
vectorul obținut prin evaluarea polinomului în rădăcinile unității:

\begin{equation}
  \label{dft}
  \text{DFT}(A) = (A(w_{0}), A(w_{1}), \ldots, A(w_{n-1})) \\
                = (A(w_{0}^{1}), A(w_{0}^{2}), \ldots, A(w_{0}^{n-1}))
\end{equation}

Similar, având vectorul din partea dreapta a ecuației \ref{dft}, \textbf{inversa
transformării discrete Fourieri} reconstituie coeficienții polinomului $A$:
$(a_{0}, a_{1}, \ldots a_{n-1})$. Vom nota această transformare cu
$\text{InverseDFT}$. \par

O aplicație utila este înmulțirea polinoamelor. Fie $\textbf{dot}$ operația de
înmulțire scalara a doi vectori. Aceasta se bazează pe o
proprietatea a transformării, și anume:
\begin{equation}
  \text{DFT}(A * B) = \text{dot}(\text{DFT}(A), \text{DFT}(B))
\end{equation}

Aceasta proprietate este utilă, fiind la baza unui algoritm rapid de înmulțire a
polinoamelor, deoarece $\text{DFT}(A)$ și $\text{InverseDFT}(A)$ se pot calcula
în timp $O(n \log n)$.

\subsection{Transformarea Hadamard}
\label{hadamard}

\textbf{Transformarea Hadamard} este o transformare Fourier generalizată,
executând o operație liniară pe $2^{n}$ numere reale. Aceasta este echivalenta
cu o transformare discretă $n$-dimensională Fourier de mărime
$2 \times 2 \ldots \times 2$. În mod intuitiv, se înmulțesc monoame a $n$
variabile, aplicandu-se modulo $x^{2}-1$ pe fiecare variabilă. Fie $p_{i_{1}, i_{2}} \in \{0, 1\}$:
\begin{equation}
  A = a_{0} x_{0}^{p_{0, 0}} x_{1}^{p_{0, 1}} \ldots x_{n-1}^{p_{0, n-1}}
  + a_{1} x_{0}^{p_{1, 0}} x_{1}^{p_{1, 1}} \ldots x_{n-1}^{p_{1, n-1}}
  + \ldots
  + a_{n-1} x_{0}^{p_{n-1, 0}} x_{1}^{p_{n-1, 1}} \ldots x_{n-1}^{p_{n-1, n-1}}
\end{equation}

Daca în cazul \textbf{transformării Fourieri discrete} obțineam
$C_{i+j} = \displaystyle\sum\limits A_{i} B_{j}$, în cazul \textbf{transformării Hadamard} obținem
$C_{i \oplus j} = \displaystyle\sum\limits A_{i} B_{j}$, unde $\oplus$ este operația de sau exclusiv
(xor) pe biți. Intuitiv, de ce se obține operație de xor este pentru că
cele $n$ monoame reprezentau biții fiecărui număr de la $0$ la $2^{n} - 1$.
Acestora la înmulțire li se aplica modulo $x^{2} - 1$, asemănător biților în
timpul unei operații de xor, care este ca o adunare, în urma căreia se aplica
modulo $2$, totul efectuat separat pe fiecare bit în parte.

\subsection{Algoritmul Fast Walsh-Hadamard}
\label{fasthadamard}

\textbf{Algoritmul Fast Walsh-Hadamard} \cite{fwht} este un algoritm eficient pentru a
calcula \textbf{transformarea Hadamard}. O implementare naivă a transformării
necesită complexitate de timp $O(2^{2n})$, în timp ce algoritmul pe care urmează
să-l prezentam necesita doar $O(n2^{n})$. Algoritmul se bazează pe paradigma
divide et impera, împărțind o problema de mărime $2^{n}$ în doua de mărime
$2^{n-1}$, urmărind definiția recursiva a matricei Hadamard:

\begin{equation}
  \label{hadamardmatrix}
  H_{n} =
  \begin{pmatrix}
    H_{n-1} & H_{n-1}\\
    H_{n-1} & -H_{n-1}
  \end{pmatrix}
\end{equation}

Ecuația \ref{hadamardmatrix} omite factorul de normalizare de
$\frac{1}{\sqrt{2}}$, care poate fi aplicat la final.

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{FMain}{FWHT}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $n$}}{
    \If{$n = -1$}{
      \KwRet\;
    }
    offset := $2^{n - 1}$\;
    \FMain($a[0 \ldots \text{offset}]$, n - 1)\;
    \FMain($a[\text{offset} \ldots 2^{n}]$, n - 1)\;
    \For{i in $0 \ldots 2^{n}$}{
      x := $a_{i}$\;
      y := $a_{i + \text{offset}}$\;
      $a_{i}$ := x + y\;
      $a_{i + \text{offset}}$ := x - y\;
    }
    \KwRet\;
  }
  \;
\end{algorithm}

\begin{algorithm}[H]
  \DontPrintSemicolon
  \SetKwFunction{FMain}{IFWHT}
  \SetKwProg{Fn}{Function}{:}{}
  \Fn{\FMain{$a$, $n$}}{
    \If{$n = -1$}{
      \KwRet\;
    }
    offset := $2^{n - 1}$\;
    \FMain($a[0 \ldots \text{offset}]$, n - 1)\;
    \FMain($a[\text{offset} \ldots 2^{n}]$, n - 1)\;
    \For{i in $0 \ldots 2^{n}$}{
      x := $a_{i}$\;
      y := $a_{i + \text{offset}}$\;
      $a_{i}$ := x - y\;
      $a_{i + \text{offset}}$ := x + y\;
    }
    \KwRet\;
  }
  \;
\end{algorithm}

\pagebreak

\section{Algoritmul lui Coppersmith}
\label{coppersmith}

În cadrul algoritmilor de algebră liniară pentru cuplaj avem des problema de a
calcula determinanți. De obicei, aceste grafuri nu vor fi atât de dense, deci
nici matricea Edmonds (\ref{edmonds}) nu va avea multe elemente nenule.

\textbf{Algoritmul lui Coppersmith} \cite{wiedemann} este un algoritm rapid pentru calcularea
vectorilor nucleu al unei matrice într-un corp finit. Acesta este în particular
eficient pentru matricele rare, cele care nu au atât de multe elemente nenule. \\

Considerăm $A$ o matrice pătratică de mărime $n \times n$ peste un corp finit $F$, atunci:

\begin{defn}
  Polinomul caracteristic al lui $A$ este definit că
  $p(\lambda) = \det(\lambda I_{n} - A)$.
\end{defn}

\begin{thm}
  \label{cayleyhamilton}
  \textbf{Cayley-Hamilton} spune că fiecare matrice pătratica într-un inel
  comutativ își satisface ecuația caracteristica, mai exact $p(A) = 0$.
\end{thm}

$A$ are un polinom monic $P$ peste corpul $F$ de grad minim pentru care
$P(A) = 0$. $P$ se numește polinomul minim al lui $A$. În
cazul matricelor rare, ne așteptam că acest polinom să aibă un grad mai mic
decât cel al polinomului caracteristic. $P$ mereu va divide polinomul
caracteristic, iar din \ref{cayleyhamilton}, gradul acestuia nu va fi mai mare
de $n$. Fie $P = a_{0} + a_{1}x + \ldots + a_{n'}x^{n'}$, unde $n' \leq n$.

\begin{lem}
  \label{polminimdet}
  Dacă $n' = n$, atunci $P$ este chiar polinomul caracteristic al lui $A$, iar
  determinantul este chiar $(-1)^{n}P(0)$.
\end{lem}

\begin{lem}
  Putem folosi polinomul minimal pentru a calcula determinantul matricei $A$.
\end{lem}

\begin{proof}
  Fie $d$ un vector de $n$ elemente
  alese uniform aleator și independent din $F$, iar
  $D =
  \begin{bmatrix}
    d_{1} & 0 & \dots \\
    0 & d_{2} & \dots \\
    \vdots & \ddots & \\
    0 & \dots & d_{n}
  \end{bmatrix}$. Conform lemei Schwartz-Zippel, matricea $AD$ este singulară cu
  probabilitate de cel mult $\frac{n}{|F|}$. În cazul în care nu este, atunci ne
  așteptăm ca polinomul minim să aibă grad egal chiar cu $n$. În acest caz,
  putem calcula rapid $\det(AD)$ folsind \ref{polminimdet} și apoi să aflăm
  $\det(A) = \frac{\det(AD)}{det(D)}$, iar $\det(D) = \prod_{i=1}^{n} d_{i}$.
\end{proof}

Fie $x'$ un vector de $n$ elemente alese uniform aleator și independent din $F$,
iar $x = Ax'$. Fie $S = [x, Ax, A^{2}x, \ldots]$.
Considerăm un alt vector de $n$ elemente, $y$ și următorul șir de valori din $F$: $[yx, yAx, yA^{2}x, \ldots ]$.

Știm că $P(A) = 0$, atunci $\displaystyle\sum\limits_{i=0}^{n'} a_{i} A^{i} = 0$. Obținem că și
$\displaystyle\sum\limits_{i=0}^{n'} y (a_{i} (A^{i} x)) = 0$. Intuitiv, știm că $P(S) = 0$, însă nu
știm să calculam eficient o secvență $b_{0}, b_{1}, \ldots b_{k}$ astfel încât
$\displaystyle\sum\limits_{i=0}^{k} b_{i} S_{i+o} = 0 \ \forall \ o \geq 0$, pentru că elementele din $S$ sunt vectori, însă
atunci când reducem la cazul cu valori, există o șansă destul de mare ca secvență $b$
care anihilează $yS$ să anihileze și $S$, deci
$\displaystyle\sum\limits_{i=0}^{k} b_{i} M^{i}x = 0$.
Înlocuind cu $x = Ax'$, obținem că $M \displaystyle\sum\limits_{i=0}^{k} b_{i} M^{i} x' = 0$, deci
dacă $b_{i} M^{i} x'$ este nenul, atunci am obținem un nucleu al lui $M$.

Pentru a calcula elementele secvenței $S$ se folosesc algoritmi pentru găsirea
\textbf{polinomului minim al unei recurențe liniare}. Algoritmul
\textbf{Berlekamp Massey} dacă $|F|$ este un număr prim (doua corpuri de
aceeași mărime sunt izomorfe), altfel, se factorizează
$|F| = p_{1}^{k_{1}} p_{2}^{k_{2}} \ldots p_{t}^{k_{t}}$ și se rezolvă $t$
probleme cu $|F_{i}'| = p_{i}^{k_{i}}$ cu algoritmul \textbf{Reeds-Sloane}, iar
rezultatele se combina folosind teorema chineză a resturilor. În ce urmează vom
prezenta un alt algoritm echivalent care nu folosește împărțiri, deci nu este
constans la un tip de corp anume \cite{sugiyama}.

\pagebreak

\section{Polinomul minim al unei recurențe liniare}

\begin{defn}
  O secvență infinita $a$ cu elemente dintr-un corp $F$ se numește recurență
  liniară dacă și numai daca exista constantele $c_{1}, c_{2}, \ldots, c_{k}$
  astfel încât
  $a_{t} = a_{t-1}c_{1} + a_{t-2}c_{2} + \ldots + a_{t-k}c_{k} \ \forall \ t > k$.
\end{defn}

\begin{defn}
  Pentru o recurența liniară $a$ și $c_{0}, c_{1}, \ldots c_{k} \in F$, astfel
  încât
  $c_{0} a_{t} = a_{t-1}c_{1} + a_{t-2}c_{2} + \ldots + a_{t-k}c_{k} \ \forall \ t > k$,
  atunci polinomul $c_{0} x^{k} + c_{1} x^{k-1} + \ldots + c_{k} x^{0}$ se
  numește anihilator al lui $a$.
\end{defn}

\begin{defn}
  Un ideal al unui inel $R$ generat de $S \subset R$ este
  $\{f = \displaystyle\sum\limits_{i=1}^{r} x_{i}s_{i} \ | \ r \in \mathbb{N}, \ s_{i} \in S, x_{i} \in R\}$.
\end{defn}

\begin{defn}
  Un ideal $<S>$ dintr-un inel $R$ este de tip finit dacă admite un sistem finit de generatori:
  $\exists \ s_{1}, s_{2}, \ldots s_{r} \in \ $  astfel încât
  $\forall \ s \in \ , s = \displaystyle\sum\limits_{i=1}^{r} x_{i} s_{i}, \ x_{i} \in R$.
\end{defn}

\begin{defn}
  Un ideal se numește principal dacă admite un sistem de generatori format
  dintr-un singur element.
\end{defn}

\begin{defn}
  Inelul $R$ se numește principal dacă orice ideal este principal.
\end{defn}

\begin{thm}
  $F[x]$ este un inel principal.
\end{thm}

\begin{lem}
  Anihilatoarele lui $a$ formează un ideal în $F[x]$.
\end{lem}

\begin{clr}
  Idealul format din anihilatoarele lui $a$ este generat de un polinom monic de
  grad minim. Acesta se numește polinomul minim al lui $a$.
\end{clr}

\begin{thm}
  Funcția generatoare $A(x) = \displaystyle\sum\limits_{n=0}^{\infty} a_{n}x^{n}$ este rațională:
  $A(x) = \frac{P(x)}{Q(x)}$ unde $deg(P) < k$, iar $Q(x) = 1 - \displaystyle\sum\limits_{i=1}^{k} c_{i}x^{i}$.
\end{thm}

\begin{thm}
  Dacă știm că gradul polinomului minim este cel mult $m$, atunci acesta este
  unic determinat de primele $2m$ elemente ale lui $a$; sau altfel spus
  $P(x) \equiv Q(X) A(X) \mod x^{2m}$.
\end{thm}

\noindent \textbf{Identitatea lui Bezout}: dacă $g$ este cel mai mare divizor comun al
polinoamelor $a$ și $b$, atunci există două polinoame $u$ și $v$ astfel încât
$ua + vb = g$.

În \cite{sugiyama} se arată că putem executa algoritmul lui Euclid extins pentru
calcularea valorilor $u$ și $v$, alegând $a = A(x) \mod x^{2m}$, iar
$b = x^{2m}$ până când $deg(g) < m$.

\pagebreak

\section{Filtrare prin rădăcini ale unității}

\label{rootsofunityfilter}

\textbf{Filtrarea prin rădăcini ale unității} \cite{rootsofunityfilter} este o tehnică de izolare și
însumare a coeficienților unui polinom. Aceasta se pretează, în particular,
atunci când indexurile sunt periodice modulo $n$.

\begin{thm}
  Dacă $p$ este un polinom și $\zeta$ este o rădăcină a unității de ordin
  $n$ ($\zeta^{n} = 1$), atunci $\frac{1}{n} \displaystyle\sum\limits_{i=0}^{n-1} p(\zeta^{i})$
  este suma indexurilor dizibile prin $n$.
\end{thm}

\begin{proof}
  Fie $p = a_{0}x^{0} + a_{1}x^{1} + \ldots + a_{k}x^{k}$. Consideram contribuția
  unui index $p$, nedivizibil prin $n$ la suma
  $\displaystyle\sum\limits_{i=0}^{n-1} p(\zeta^{i}) = a_{p}(\zeta^{0} + \zeta^{p} + \zeta^{2p} + \ldots + \zeta^{(n-1)p})$.
  Termenul din partea dreaptă este de fapt $\displaystyle\sum\limits_{i=0}^{n-1} \zeta^{i*p} = \frac{\zeta^{p*n} - 1}{\zeta^{p} - 1} = 0$,
  deci se anulează. Celelalte indexuri vor avea coeficient $1$ pentru fiecare
  $\zeta^{i}$, deci se vor aduna în total de $n$ ori, motiv pentru care toată suma
  se împarte la $n$.
\end{proof}


\pagebreak

\section{Rădăcini primitive}
\label{primitiveroot}

\cite{primitiveroot} În ce urmează presupunem că lucrăm într-un corp finit $\mathbb{F}_{p}$, unde $p$ este un număr prim.

\begin{defn}
  Un număr $g$ se numește rădăcină primitiva modulo $n$ dacă și numai dacă pentru orice $a$ coprim cu $p$ există un alt număr
  $l$ astfel încât $g^{l} = a \mod p$.
\end{defn}

\begin{thm}
  \label{carmichael}
  (Funcția Carmichael) Fie $g$ o rădăcină primitivă, atunci cel mai mic număr $l$ astfel încât $g^{l} = 1$ este $\phi(p)$.
\end{thm}

\begin{thm}
  \label{carmichaelr}
  Reciproca teoremei \ref{carmichael} este adevărată și stă la baza algoritmului pentru aflarea generatorului.
\end{thm}

Un algoritm evident pentru a caută rădăcini primitive încearcă numerele din $\mathbb{F}_{p}$ pe rând și apoi le calculează
puterile pentru a verifica dacă sunt diferite. Deși în general rădăcină primitivă este un număr destul de mic, acest algoritm
nu este cel mai eficient posibil.

Din \ref{carmichaelr} știm că, pentru un $a$ fixat, dacă găsim un $l < \phi(p)$ pentru care $a^{l} = 1$, atunci acesta nu este un
generator. Din \textbf{teorema lui Euler} știm că orice $a$ coprim cu $p$ satisface $a^{\phi(p)} = 1 \mod p$. Aceste observații pot
îmbunătăți algoritmul menționat anterior, însă, pentru că noi îl folosim pentru $p$ număr prim, atunci $\phi(p) = p - 1$, deci nu
ajută foarte mult. \textbf{Teorema lui Lagrange} mai spune că este suficient să verificam că $a^{d} \neq 1 \mod p$ pentru toți $d \ |\ \phi(p)$.
Aceasta este deja o îmbunatățire foarte mare, deoarece numărul de divizori ai lui $\phi(p)$ este de ordinul $O(p^{\frac{1.066}{\ln \ln p}})$.

Dacă $\phi(p) = a_{1}^{b_{1}} a_{2}^{b_{2}} \ldots a_{t}^{b_{t}}$, atunci este de ajuns să se verifice acei $d = \frac{\phi(p)}{a_{i}}$.

\begin{proof}
  Presupunem că există un divizor propriu $d$ al lui $\phi(p)$ astfel încât $g^{d} = 1 \mod p$. Atunci există un factor prim $a_{i}$
  din factorizarea lui $\phi(p)$ astfel încât $d \ | \ \frac{\phi(p)}{a_{i}}$ și evident $g^{\frac{\phi(p)}{a_{i}}} = 1 \mod p$.
\end{proof}

Complexitatea de timp a acestei soluții este acum $O(g \log^{2} p)$, iar dacă presupunem ipoteza Riemann ca fiind adevărată, $g = O(\log^{6} p)$.
